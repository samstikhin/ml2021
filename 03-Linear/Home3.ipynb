{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Матричные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются вектор $\\boldsymbol{x}$ длины $n$, $\\boldsymbol{c}$ длины $m$ и матрица $A$ размера $\\{n\\times m\\}$\n",
    "\n",
    "Пусть $y = \\boldsymbol{x}^TA\\boldsymbol{c}$\n",
    "\n",
    "Найдите $\\frac{dy}{d\\boldsymbol{x}}$, $\\frac{dy}{dA}$ и $\\frac{dy}{d\\boldsymbol{c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "def matrix_deriv(x: np.array, A: np.array, c: np.array):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSDYNrQmWK6T"
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "x = np.array([ [1], [2], [3],  [4]])\n",
    "c = np.array([[25], [5],[-9]])\n",
    "A = np.arange(12).reshape(4,3)\n",
    "\n",
    "y1, y2, y3 = matrix_deriv(x, A, c)\n",
    "\n",
    "assert_array_equal(y1, np.array([[-13],\n",
    "                                [ 50],\n",
    "                                [113],\n",
    "                                [176]]))\n",
    "\n",
    "assert_array_equal(y2, np.array([[ 25,   5,  -9],\n",
    "                                [ 50,  10, -18],\n",
    "                                [ 75,  15, -27],\n",
    "                                [100,  20, -36]]))\n",
    "                   \n",
    "assert_array_equal(y3, np.array([[60, 70, 80]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# SLIDE (1) Честная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "В случае линейной регрессии по функционалу $MSE$ задачу оптимизации можно записать следующим образом:\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{k=1}^N (\\boldsymbol{x}_k^T\\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "Мы уже знаем, что решение будет выглядеть следующим образом:\n",
    "\n",
    "$$\\boldsymbol{w} = \\begin{cases} X^{-1}\\boldsymbol{y}, & n = m\\\\\n",
    "(X^TX)^{-1}X^T\\boldsymbol{y}, & n > m\\\\\n",
    "X^{T}(XX^{T})^{-1}\\boldsymbol{y}, & n < m\n",
    "\\end{cases}$$\n",
    "\n",
    "где $n$ - количество объектов, а $m$ - количество признаков + 1 (дополнительно 1 вес без признака).\n",
    "\n",
    "Решите задачу регрессии честно, используя формулу выше. \n",
    "\n",
    "Чтобы вы не запутались мы сразу написали добавление единичного столбца к матрице $X$.\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1]])\n",
    "y_train = np.array([2])\n",
    "X_test = np.array([[0.], [2.], [3.]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([1., 3., 4.])\n",
    "model.w_ == np.array([[1.], [1.]])\n",
    "model.coef_ == np.array([1., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "class TrueLinReg():\n",
    "    def __init__(self):\n",
    "        self.w_ = None  # столбец\n",
    "        self.coef_ = None # строка\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) #добавляем вес без признака\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w_).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [3], [4]])), np.array([0, 3, 4]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2], [3]])\n",
    "y_reg = np.array([1, -2, 1])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0],[4]])), np.array([0., 0.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1]])\n",
    "y_reg = np.array([2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0.], [2.], [3.]])), np.array([1., 3., 4.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [1.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 1.]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=9, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=15, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Честная регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте регуляризацию с коэффициентом $\\lambda$ и решите задачу регрессии (для любого соотношения $m$ и $n$ используйте формулу)\n",
    "$$\\boldsymbol{w} = (X^TX + \\lambda E)^{-1}X^T\\boldsymbol{y}$$\n",
    "где $E$ - единичная матрица.\n",
    "\n",
    "Не забудьте сами добавить **справа** единичный столбец к матрице $X$ аналогично предыдущей задаче.\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "lamb = 1\n",
    "\n",
    "X_test = np.array([[0], [4]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0.33, 3])\n",
    "model.w_ == np.array([[0.67], [0.33]])\n",
    "model.coef_ == np.array([0.67, 0.33])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueReg():\n",
    "    def __init__(self, lamb):\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [4]])), np.array([0.33, 3]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.67], [0.33]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0.67, 0.33]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=99, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=2000, n_features=10, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=150, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsyTwOrXZmOI"
   },
   "source": [
    "# SLIDE (1) Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOHHlozhZmOP"
   },
   "source": [
    "На вход подаются обучающая выборка $(X,\\boldsymbol{y})$ и как-то определенные веса $w$. Верните градиент функции $MSE$ для изменения весов в алгоритме градиентного спуска.\n",
    "$$\\nabla_{\\boldsymbol{w}} L = \\frac{2}{n} X^{T}(X\\boldsymbol{w} - \\boldsymbol{y})$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[0.2], [0.2]]) #столбец!\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[-8.4], \n",
    "       [-8.4]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLzhM5-LZmOX"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XegQEY5ZmOc"
   },
   "outputs": [],
   "source": [
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YLSZH2_ZmO5"
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.2], \n",
    "              [0.2]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]])\n",
    "assert_array_almost_equal( gradient_step(w, X, y), np.array([[-5.6],[-5.6]]))\n",
    "######################################################\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "\n",
    "######################################################\n",
    "g = np.array([-1.0993842672390932, -3.0877233608129395, -2.13575686356165, -0.21916813495587567, \n",
    "              1.6843662270764397, 3.3322365301217958, -5.049140901669979, 1.5001184420835205, \n",
    "              -0.4856689267508395, 4.84907077228488, 1.8736602701312628, -2.8537960600078045, \n",
    "              5.534114292930919, -0.6315258126682273, -0.10153263395507897, 0.10430238830850164, \n",
    "              -0.6327318737851857, -3.3518163054239145, -1.9647292933263618, 3.1983699740883607])\n",
    "\n",
    "assert np.allclose(gradient_step(w, X, y).reshape(20), g)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4PvDGpRZmPU"
   },
   "source": [
    "# SLIDE (1) Стохастический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdPhYgwyZmPV"
   },
   "source": [
    "Реализуйте стохаистический градиентный спуск: пересчитайте $\\nabla_{\\boldsymbol{w}}L$ только для одного случайно выбранного элемента из выборки $X$.\n",
    "\n",
    "$$L_{k} = (\\boldsymbol{x}_k^T \\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{w}}L_{k} = \\boldsymbol{x}_k (\\boldsymbol{x}_k^T \\boldsymbol{w}  - y_k)$$\n",
    "где $\\boldsymbol{x}_i$ - вектор объекта выборки, выбранный случайно.  \n",
    "\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице $X$, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1], [1]]) #столбец!\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "y = np.array([[1], [0]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[0.],\n",
    "       [0.],\n",
    "       [0.]]) #столбец!\n",
    "или\n",
    "array([[2.],\n",
    "       [2.],\n",
    "       [2.]]) #столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuVTpgPEZmPW"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxwmdI1VZmPX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goRVoRRpZmPe"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1], [1]])\n",
    "y = np.array([[1], [0]])\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "\n",
    "z , k = 0 , 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 2:\n",
    "        z += 1\n",
    "    else:\n",
    "        k += 1\n",
    "assert z >= 35 and k >= 35\n",
    "######################################################\n",
    "w = np.array([[5], [2], [1], [5]])\n",
    "y = np.array([[0.5], [2], [-3]])\n",
    "X = np.array([[1, 1, 1, 1], [0, 0, 0, 1], [3, 5, 3, 1]])\n",
    "\n",
    "z , k, f = 0 , 0, 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 108:\n",
    "        z += 1\n",
    "    elif g[0] == 0:\n",
    "        k += 1\n",
    "    else:\n",
    "        f += 1\n",
    "assert z >= 25 and k >= 25 and f >= 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd1Jg4PKZmM1"
   },
   "source": [
    "# SLIDE (1) Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1FBAmquZmM7"
   },
   "source": [
    "Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями. \n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L$$\n",
    "\n",
    "Используйте параметры `max_iter`=$1000$, `eta`=$0.1$\n",
    "\n",
    "Все итерации проходить не нужно. Остановитесь в момент, когда норма разницы между старыми и новыми весами станет меньше $1e-9$. \n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3],[4]]))\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3., 4.])\n",
    "model.coef_ = np.array([1., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmlBx1vYZmNE"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F1cIF6-ZmNL"
   },
   "outputs": [],
   "source": [
    "class GDLinReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.1):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.coef_ = None #строка\n",
    "        self.w_ = None #столбец\n",
    "    \n",
    "    def _gradient_descending(w, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros((m, 1)) #столбец\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n",
    "        return (X @ self.w_).reshape(-1) #возвращаем всегда строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3., 4.]), decimal=1)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=1)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model2 = GDLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "coef_my = model2.coef_\n",
    "\n",
    "assert mean_absolute_error(coef_my, coef_real) < 5\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krP0hQnFZmPA"
   },
   "source": [
    "# SLIDE (1) Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwUtXwj_ZmPD"
   },
   "source": [
    "Реализуйте L2-регуляризацию (так же известную как Ridge).\n",
    "\n",
    "$$L = \\frac{1}{2}\\lVert X\\boldsymbol{w} - \\boldsymbol{y}\\rVert_2^2 + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2$$\n",
    "\n",
    "Найдите новый $\\nabla_{\\boldsymbol{w}} L$ и верните его.\n",
    "\n",
    "Обратите внимание, что свободный коэффициент весов (самый последний) **не нужно** регуляризовывать.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "np.array([[28.], \n",
    "          [27.]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3W_KzN3ZmPF"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kN24jsqZmPH"
   },
   "outputs": [],
   "source": [
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    # Коэффициент регуляризации lamb\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRDN31FAZmPQ"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(gradient_step_l2(w, X, y, 1), np.array([[28.], [27.]]))\n",
    "\n",
    "def gradient_descending(w, X, y, l):\n",
    "    # посчитайте градиент функции ошибок по весам. Переменная l=0, не используйте её.\n",
    "    g = X.T @ (X @ w - y)\n",
    "    return g\n",
    "\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "######################################################\n",
    "nw = np.copy(w)\n",
    "nw[-1] = 0\n",
    "l = 1\n",
    "assert np.allclose(gradient_descending(w, X, y, 0) + l * nw, gradient_step_l2(w, X, y, l))\n",
    "l = 123\n",
    "assert np.allclose(gradient_descending(w, X, y, 0) + l * nw, gradient_step_l2(w, X, y, l))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Логистический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику $LogLoss$:\n",
    "$$\\ln{L} = -\\sum_{k=1}^{n}y_i\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))}$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$\n",
    "\n",
    "Ваша задача взять **производную** этой функции и вернуть вектор градиентов $\\nabla_{\\boldsymbol{w}}\\ln{L}$. Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n",
    "\n",
    "Обратите внимание: \n",
    "\n",
    "* $w$, $y$ - столбцы, а не строки. Ответ тоже столбец.\n",
    "* В функции используется **натуральный** логарифм.\n",
    "* Нужно посчитать полный градиент, а не стохастический.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([0, 0])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[2], \n",
    "       [2]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [0]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), np.array([[2], [2]]))\n",
    "######################################################\n",
    "w = np.array([[-1.90059439], [-0.39772511], [-0.41752917], [ 1.52698708],\n",
    "              [ 1.93384661], [ 0.55038402], [-0.43914043], [-0.53450491],\n",
    "              [-0.20436079], [ 1.47930748], [-0.43431298], [-0.67715663],\n",
    "              [ 0.6140469 ], [ 0.33724021], [-0.47389417], [-0.27931867],\n",
    "              [-0.19815547], [-1.10817221], [-0.2783202 ], [-0.26673129]])\n",
    "\n",
    "y = np.array([[-1.35285676], [-1.90639143], [-1.1872863 ], [-0.47531246],\n",
    "              [-0.5658066 ], [-2.01777789], [-0.55111576], [-1.17508504],\n",
    "              [-0.31591632], [-1.10876769]])\n",
    "\n",
    "X = np.array([[-1.79418438, -0.28247459,  0.63485396,  0.18973693, -1.13052678,\n",
    "                -0.70693165,  0.02485169, -1.32269407, -1.19972889, -0.90835517,\n",
    "                1.70735351, -0.31278511,  0.26195237,  1.15186451, -1.06028399,\n",
    "                0.35805288,  0.1097917 , -1.64881113, -0.26764338,  0.52419335],\n",
    "              [-0.39820007, -1.07706627,  0.03495314, -0.36069043, -0.38655026,\n",
    "                0.99804715,  0.62738777,  1.53889357,  0.47893302, -1.17044503,\n",
    "                -1.14344307,  0.99980883, -0.89283077, -0.30342722, -1.64897791,\n",
    "                -1.13511464, -0.82071543,  1.8879236 , -1.5129215 ,  0.12872281],\n",
    "              [-0.26032624, -0.59442306, -0.20607555,  1.0785192 , -0.72817743,\n",
    "                1.60260617, -2.34932775,  0.55693193,  0.25573628,  1.53147877,\n",
    "                1.53834672,  0.1878024 ,  1.18042765,  0.54514643,  0.01803304,\n",
    "                1.51429761, -0.17469223,  0.81986536, -0.46578708,  0.42436921],\n",
    "              [ 0.10641183, -0.18608143,  2.69988291,  0.30114113, -0.56763953,\n",
    "                -0.89837188,  1.19577105, -0.86920194, -2.68921283, -0.58902866,\n",
    "                -0.14408206, -1.84817003, -0.9124478 ,  0.55288917,  1.14731704,\n",
    "                -0.4114787 , -0.53848335,  0.63225441,  0.61095749, -0.16311904],\n",
    "              [ 0.42409436,  1.00368214, -1.47558928,  0.01692064,  0.95318017,\n",
    "                -0.04567367, -1.62167744, -0.20345068, -0.1389324 , -0.4342349 ,\n",
    "                0.41877207, -1.37897995, -0.31973986, -1.6112183 , -0.18485078,\n",
    "                -1.16219592, -1.08054026, -0.49210254, -1.52373135,  2.34733614],\n",
    "              [ 0.19339171, -1.25994886, -1.3263278 , -1.56807527, -1.16484596,\n",
    "                1.51848938, -0.40378262,  0.28889234,  0.10067917,  1.89112864,\n",
    "                -1.87355328,  0.19510271,  1.13826496,  0.41762094,  0.61912712,\n",
    "                -1.06733394, -0.22801313, -1.49211563, -0.97819834, -0.08353847],\n",
    "              [ 0.5225887 ,  0.03288573, -0.1318777 ,  0.75014314, -1.36193554,\n",
    "                -0.71094454,  2.09153476,  1.50715026,  0.03406876, -0.71503137,\n",
    "                1.22186058,  0.9721076 , -0.33217577,  0.60094908, -0.19959933,\n",
    "                -0.79264798, -0.80242992, -1.46592485,  0.00955109, -0.31271658],\n",
    "              [ 1.07854603,  1.63387859, -0.68689403, -1.49026312, -0.05352863,\n",
    "                0.74923652, -0.61221446,  0.25499529, -0.87862293,  2.30763454,\n",
    "                -0.45654804, -0.49537167, -1.14185717,  0.09358794,  1.71154247,\n",
    "                0.51549091, -0.26392429,  1.16472574, -0.53397584, -0.79062073],\n",
    "              [-0.56480156, -1.83123029,  0.99887682, -0.60648359,  0.37281722,\n",
    "                0.19015213,  0.08486153,  1.54633375, -0.22768608,  1.4061507 ,\n",
    "                -0.07928461, -1.03523357,  0.44072784, -0.98970029, -0.10722007,\n",
    "                1.54788238, -0.89526339, -1.14606415, -0.65972113,  0.96906237],\n",
    "              [ 1.36608419,  0.09914266, -0.02822534,  0.97303833,  2.14361812,\n",
    "                0.18005144,  0.94756695,  1.97038803, -1.00026315,  0.47603231,\n",
    "                1.14925524, -1.16030033,  1.42723652,  0.00458275,  0.62078139,\n",
    "                -2.47565446, -0.16698078,  0.25653049,  0.8664139 , -0.37928587]])\n",
    "\n",
    "g = np.array([[-0.43431449],\n",
    "       [-6.4967537 ],\n",
    "       [-3.33915541],\n",
    "       [-2.60685525],\n",
    "       [-3.13767717],\n",
    "       [ 8.9115332 ],\n",
    "       [-4.30338709],\n",
    "       [ 8.52721835],\n",
    "       [-6.04197056],\n",
    "       [ 8.94208373],\n",
    "       [ 2.41288223],\n",
    "       [-4.65486414],\n",
    "       [ 5.81341316],\n",
    "       [ 1.54813258],\n",
    "       [-0.23116471],\n",
    "       [-6.1655314 ],\n",
    "       [-6.56704074],\n",
    "       [-3.53911584],\n",
    "       [-9.10686695],\n",
    "       [ 4.83364865]])\n",
    "\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), g)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M2uSfHyZmPh"
   },
   "source": [
    "# SLIDE (1) Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocEDVotnZmPi"
   },
   "source": [
    "Теперь реализуем Логистическую регрессию на стохастическом градиентном спуске.\n",
    "$$L = -\\ln{Likelihood} = -\\sum_{k=1}^{n}y_k\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2 \\rightarrow \\min$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$, $\\lVert \\boldsymbol{w} \\rVert_2^2 = \\sum_{i=1}^{m} w_i^2$= - квадрат эвклидовой нормы\n",
    "\n",
    "Собственно вероятность принадлежности определенному классу:\n",
    "$$P(y_k=1) = \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})~~~P(y_k=0) = 1 - \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})$$\n",
    "\n",
    "Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса сгенерированны рандомно. \n",
    "\n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L_k$$\n",
    "\n",
    "Будьте внимательны:\n",
    "\n",
    "* На вход алгоритму приходит $\\boldsymbol{y}$ - **строка**, как и в любой sklearn алгоритм.\n",
    "* Не устанавливайте количество итераций больше 1000, так как алгоритм будет долго работать.\n",
    "* Как и в линейной регрессии не забудьте добавить единичный столбец справа.\n",
    "\n",
    "Можете использовать и обычный спуск, главное чтоб по времени зашло.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = SGDLogReg().fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n",
    "y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([1., 0.])\n",
    "y_prob = np.array([[0.1, 0.9],  # это не точный ответ, но очень приблизительный, отличие на 0.1 - это нормально\n",
    "                   [0.9, 0.1]]) # для каждого объекта возвращаем его вероятность нуля и единицы\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FR5V2Xf2KgaG"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tI7mwEgKgZ3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLogReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.01, lamb = 1):\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.lamb = lamb\n",
    "        self.w_ = None\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def _stochastic_step(self, w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        #модифицируйте функцию из предыдущей задачи\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros(shape=(m, 1))\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "model = SGDLogReg().fit(X_clf, y_clf)\n",
    "\n",
    "assert model.lamb == 1\n",
    "\n",
    "assert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\n",
    "assert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n",
    "######################################################\n",
    "np.random.seed(1337)\n",
    "n = 200\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=2, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.01).fit(X_train, y_train)\n",
    "model_real = LogisticRegression(C=0.01, solver='sag').fit(X_train, y_train)\n",
    "\n",
    "assert model.lamb == 0.01\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n",
    "######################################################\n",
    "np.random.seed(1228)\n",
    "n = 1000\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=1, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.5).fit(X_train, y_train)\n",
    "model_real = SGDClassifier(loss='log',alpha=0.5).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "assert model.lamb == 0.5\n",
    "\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.2\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.25\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ \\boldsymbol{g}^{(t)} = \\gamma \\boldsymbol{g}^{(t - 1)} + \\eta \\nabla_{\\boldsymbol{w}}L_{k}$$\n",
    "$$ \\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\boldsymbol{g}^{(t)}$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\eta$ — learning rate\n",
    " - $\\boldsymbol{w}$ — вектор параметров\n",
    " - $\\boldsymbol{g}$ — вектор градиентов \n",
    " - $L$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient #функция находящая градиент за вас\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            '''\n",
    "            .∧＿∧ \n",
    "            ( ･ω･｡)つ━☆・*。 \n",
    "            ⊂  ノ    ・゜+. \n",
    "            しーＪ   °。+ *´¨) \n",
    "                    .· ´¸.·*´¨) \n",
    "                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "            '''\n",
    "            self.w = \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "         1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "r1 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=0.1)\n",
    "r1.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r1.fit(X, y)\n",
    "######################################################\n",
    "assert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165]))\n",
    "assert np.allclose(r1.w.reshape(6), np.array([-0.05887894, -0.11775788, -0.17663682, -0.11775788, -0.05887894,  0.05887894]))\n",
    "######################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
