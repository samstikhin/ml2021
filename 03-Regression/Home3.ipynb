{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# 1. Матричные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются вектор $x$ длины $n$, $c$ длины $m$ и матрица $A$ размера $\\{n\\times m\\}$\n",
    "\n",
    "Пусть $y = x^TAc$\n",
    "\n",
    "Найдите $\\frac{dy}{dx}$, $\\frac{dy}{dA}$ и $\\frac{dy}{dc}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "def matrix_deriv(x: np.array, A: np.array, c: np.array):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSDYNrQmWK6T"
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "x = np.array([ [1], [2], [3],  [4]])\n",
    "c = np.array([[25], [5],[-9]])\n",
    "A = np.arange(12).reshape(4,3)\n",
    "\n",
    "y1, y2, y3 = matrix_deriv(x, A, c)\n",
    "\n",
    "assert_array_equal(y1, np.array([[-13],\n",
    "                                [ 50],\n",
    "                                [113],\n",
    "                                [176]]))\n",
    "\n",
    "assert_array_equal(y2, np.array([[ 25,   5,  -9],\n",
    "                                [ 50,  10, -18],\n",
    "                                [ 75,  15, -27],\n",
    "                                [100,  20, -36]]))\n",
    "                   \n",
    "assert_array_equal(y3, np.array([[60, 70, 80]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "# 2. Честная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "В случае линейной регрессии  по функционалу $MSE$ задачу оптимизации можно записать следующим образом:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\langle w, x_i \\rangle - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "Мы уже знаем, что решение будет выглядеть следующим образом:\n",
    "\n",
    "$$W = \\begin{cases} X^{-1}Y, & n = m\\\\\n",
    "(X^TX)^{-1}X^Ty, & n > m\\\\\n",
    "X^{T}(XX^{T})^{-1}y, & n < m\n",
    "\\end{cases}$$\n",
    "\n",
    "где $n$ - количество объектов, а $m$ - количество признаков + 1 (дополнительно 1 вес без признака).\n",
    "\n",
    "Решите задачу регресии честно, используя формулу выше. \n",
    "\n",
    "Чтобы вы не запутались мы сразу написали добавление единичного столбца к матрице $X$.\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1]])\n",
    "y_train = np.array([2])\n",
    "X_test = np.array([[0.], [2.], [3.]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([1., 3., 4.])\n",
    "model.w_ == np.array([[1.], [1.]])\n",
    "model.coef_ == np.array([1., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "class TrueLinReg():\n",
    "    def __init__(self):\n",
    "        self.w_ = None  #cтолбец \n",
    "        self.coef_ = None #строка\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) #добавляем вес без признака\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [3], [4]])), np.array([0, 3, 4]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2], [3]])\n",
    "y_reg = np.array([1, -2, 1])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0],[4]])), np.array([0., 0.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1]])\n",
    "y_reg = np.array([2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0.], [2.], [3.]])), np.array([1., 3., 4.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [1.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 1.]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=9, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=3)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=15, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Честная регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте регуляризацию с коэффициентом $\\lambda$ и решите задачу регрессии (для любого соотношения $m$ и $n$ используйте формулу)\n",
    "$$w = (X^TX + \\frac{\\lambda}{2}I)^{-1}X^Ty$$\n",
    "где $I$ - единичная матрица.\n",
    "\n",
    "Не забудьте сами добавить **справа** единичный столбец к матрице $X$ аналогично предыдущей задачи.\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "lamb = 1\n",
    "\n",
    "X_test = np.array([[0], [4]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0.33, 3])\n",
    "model.w_ == np.array([[0.67], [0.33]])\n",
    "model.coef_ == np.array([0.67, 0.33])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueReg():\n",
    "    def __init__(self, lamb):\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [4]])), np.array([0.33, 3]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.67], [0.33]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0.67, 0.33]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=99, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=2000, n_features=10, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=150, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsyTwOrXZmOI"
   },
   "source": [
    "# 4. Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOHHlozhZmOP"
   },
   "source": [
    "На вход подаются обучающая выборка $(X,y)$ и как-то определенные веса $w$. Верните градиент функции $MSE$ для изменения весов в алгоритме градиентного спуска.\n",
    "$$\\nabla L = X^{T}(Xw - y)$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[0.2], [0.2]]) #столбец!\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[-8.4], \n",
    "       [-8.4]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLzhM5-LZmOX"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XegQEY5ZmOc"
   },
   "outputs": [],
   "source": [
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YLSZH2_ZmO5"
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.2], \n",
    "              [0.2]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]])\n",
    "assert_array_almost_equal( gradient_step(w, X, y), np.array([[-8.4], [-8.4]]))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4PvDGpRZmPU"
   },
   "source": [
    "# 5. Стохастический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdPhYgwyZmPV"
   },
   "source": [
    "Реализуйте стохаистический градиентный спуск: пересчитайте $\\nabla_{w}L$ только для одного случайно выбранного элемента из выборки $X$.\n",
    "\n",
    "$$L_{i} = (x_i^T w - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "$$\\nabla_{w}L^{i} = x_i (x_i^T w  - y_i)$$\n",
    "где $x_i$ - вектор объекта выборки, выбранный случайно.  \n",
    "\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице $X$, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1], [1]]) #столбец!\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "y = np.array([[1], [0]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[0.],\n",
    "       [0.],\n",
    "       [0.]]) #столбец!\n",
    "или\n",
    "array([[2.],\n",
    "       [2.],\n",
    "       [2.]]) #столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuVTpgPEZmPW"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxwmdI1VZmPX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goRVoRRpZmPe"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1], [1]])\n",
    "y = np.array([[1], [0]])\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "\n",
    "z , k = 0 , 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 2:\n",
    "        z += 1\n",
    "    else:\n",
    "        k += 1\n",
    "assert z >= 35 and k >= 35\n",
    "######################################################\n",
    "w = np.array([[5], [2], [1], [5]])\n",
    "y = np.array([[0.5], [2], [-3]])\n",
    "X = np.array([[1, 1, 1, 1], [0, 0, 0, 1], [3, 5, 3, 1]])\n",
    "\n",
    "z , k, f = 0 , 0, 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 108:\n",
    "        z += 1\n",
    "    elif g[0] == 0:\n",
    "        k += 1\n",
    "    else:\n",
    "        f += 1\n",
    "assert z >= 25 and k >= 25 and f >= 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd1Jg4PKZmM1"
   },
   "source": [
    "# 6. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1FBAmquZmM7"
   },
   "source": [
    "Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями. \n",
    "$$w_{new} = w - \\eta\\nabla L$$\n",
    "\n",
    "Используйте параметры `max_iter`=$1000$, `eta`=$0.1$\n",
    "\n",
    "Все итерации проходить не нужно. Остановитесь в момент, когда норма разницы между старыми и новыми весами станет меньше $1e-9$. \n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3],[4]]))\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3., 4.])\n",
    "model.coef_ = np.array([1., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmlBx1vYZmNE"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F1cIF6-ZmNL"
   },
   "outputs": [],
   "source": [
    "class GDLinReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.1):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.coef_ = None #строка\n",
    "        self.w_ = None #столбец\n",
    "    \n",
    "    def _gradient_descending(w, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros((m, 1)) #столбец\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n",
    "        return (X @ self.w_).reshape(-1) #возвращаем всегда строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3., 4.]), decimal=1)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=1)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model2 = GDLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "coef_my = model2.coef_\n",
    "\n",
    "assert mean_absolute_error(coef_my, coef_real) < 5\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krP0hQnFZmPA"
   },
   "source": [
    "# 7. Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwUtXwj_ZmPD"
   },
   "source": [
    "Реализуйте L2-регуляризацию (так же известную как Ridge).\n",
    "\n",
    "$$L = \\frac{1}{2}\\lVert Xw - y\\rVert_2^2 + \\frac{\\lambda}{2}\\lVert w\\rVert_2^2$$\n",
    "\n",
    "Найдите новый $\\nabla_{w} L$ и верните его.\n",
    "\n",
    "Обратите внимание, что свободный коэффициент весов (самый последний) **не нужно** регуляризовывать.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "np.array([[28.], \n",
    "          [27.]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3W_KzN3ZmPF"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kN24jsqZmPH"
   },
   "outputs": [],
   "source": [
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    # Коэффициент регуляризации lamb\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRDN31FAZmPQ"
   },
   "outputs": [],
   "source": [
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(gradient_step_l2(w, X, y, 1), np.array([[28.], [27.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Логистический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику $LogLoss$:\n",
    "$$\\ln{L} = -\\sum_{i=1}^{n}y_i\\ln{(\\sigma(x_i^{T}w))} + (1 - y_i)\\ln{(1 - \\sigma(x_i^{T}w))}$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$\n",
    "\n",
    "Ваша задача взять **производную** этой функции и вернуть вектор градиентов $\\nabla_{w}\\ln{L}$. Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n",
    "\n",
    "Обратите внимание: \n",
    "\n",
    "* $w$, $y$ - столбцы, а не строки. Ответ тоже столбец.\n",
    "* В функции используется **натуральный** логарифм.\n",
    "* Нужно посчитать полный градиент, а не стохастический.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([0, 0])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[2], \n",
    "       [2]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [0]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), np.array([[2], [2]]))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M2uSfHyZmPh"
   },
   "source": [
    "# 9. Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocEDVotnZmPi"
   },
   "source": [
    "Теперь реализуем Логистическую регрессию на стохастическом градиентном спуске.\n",
    "$$\\ln{L} = -\\sum_{i=1}^{n}y_i\\ln{(\\sigma(x_i^{T}w))} + (1 - y_i)\\ln{(1 - \\sigma(x_i^{T}w))} + \\frac{\\lambda}{2}\\lVert w\\rVert_2^2 \\rightarrow \\min$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$, $\\lVert w\\rVert_2^2 = \\sum_{i=1}^{n} w_i^2$= - квадрат эвклидовой нормы\n",
    "\n",
    "Собственно вероятность принадлежности определенному классу:\n",
    "$$P(y_i=1) = \\sigma(x_i^T w)~~~P(y_i=0) = 1 - \\sigma(x_i^T w)$$\n",
    "\n",
    "Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса сгенерированны рандомно. \n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta\\nabla_{w} L^i$$\n",
    "\n",
    "Будьте внимательны:\n",
    "\n",
    "* На вход алгоритму приходит $y$ - **строка**, как и в любой sklearn алгоритм.\n",
    "* Не устанавливайте количество итераций больше 1000 (алгоритм будет долго работать)\n",
    "* Как и в линейной регрессии не забудьте добавить единичный столбец справа.\n",
    "\n",
    "Можете использовать и обычный спуск, главное чтоб по времени зашло.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = SGDLogReg().fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n",
    "y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([1., 0.])\n",
    "y_prob = np.array([[0.1, 0.9],  # это не точный ответ, но очень приблизительный, отличие на 0.1 - это нормально\n",
    "                   [0.9, 0.1]]) # для каждого объекта возвращаем его вероятность нуля и единицы\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FR5V2Xf2KgaG"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tI7mwEgKgZ3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLogReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.01, lamb = 1):\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.lamb = lamb\n",
    "        self.w_ = None\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def _stochastic_step(self, w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        #модифицируйте функцию из предыдущей задачи\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros(size=(m, 1))\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "model = SGDLogReg().fit(X_clf, y_clf)\n",
    "\n",
    "assert model.lamb == 1\n",
    "\n",
    "assert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\n",
    "assert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n",
    "######################################################\n",
    "np.random.seed(1337)\n",
    "n = 200\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=2, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.01).fit(X_train, y_train)\n",
    "model_real = LogisticRegression(C=0.01, solver='sag').fit(X_train, y_train)\n",
    "\n",
    "assert model.lamb == 0.01\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n",
    "######################################################\n",
    "np.random.seed(1228)\n",
    "n = 1000\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=1, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.5).fit(X_train, y_train)\n",
    "model_real = SGDClassifier(loss='log',alpha=0.5).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "assert model.lamb == 0.5\n",
    "\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{w}L^i$$\n",
    "$$ w^{(t+1)} = w^{(t)} - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\eta$ — learning rate\n",
    " - $w$ — вектор параметров\n",
    " - $L$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            '''\n",
    "            .∧＿∧ \n",
    "            ( ･ω･｡)つ━☆・*。 \n",
    "            ⊂  ノ    ・゜+. \n",
    "            しーＪ   °。+ *´¨) \n",
    "                    .· ´¸.·*´¨) \n",
    "                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "            '''\n",
    "            self.w = \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "         1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "r1 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=0.1)\n",
    "r1.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r1.fit(X, y)\n",
    "######################################################\n",
    "assert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165]))\n",
    "assert np.allclose(r1.w.reshape(6), np.array([-0.05887894, -0.11775788, -0.17663682, -0.11775788, -0.05887894,  0.05887894]))\n",
    "######################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
